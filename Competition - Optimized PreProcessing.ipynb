{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introducition\n",
    "The following code contains the creation and training of an RNN Model using Keras and TensorFlow. The project was originally started in Kaggle but was moved to Jupyter Notebook so as to utilize hardware acceleration. The aim of the model is to decide whether tweets entered are one of eight emotions. The emotions are as followed: {'joy', 'surprise', 'sadness', 'anger', 'trust', 'fear', 'disgust', 'anticipation'}. Many of the porcesses taken and utilized were thanks to the GPU-Accelerated performance of the Jupyter Notebook. With that I was able to perform more epochs as well as create more versions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-11T09:34:12.995737Z",
     "iopub.status.busy": "2022-11-11T09:34:12.994307Z",
     "iopub.status.idle": "2022-11-11T09:34:13.027580Z",
     "shell.execute_reply": "2022-11-11T09:34:13.026564Z",
     "shell.execute_reply.started": "2022-11-11T09:34:12.995536Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Importing Files\n",
    "Due to the JSON being a Nested Json, the original df did not fully contain the information needed. Due to this another dataframe was created with json_normalize so as to have the tweet_id, and the text, which were very importan. The tweet_id was necessary so as to add combine the other dataframes that contained the training and test data. The reason for this was so as to have the dataframe be able to share the tweet text via the tweet_id. Afterwards, separate dataframes where created to have the Training and Testing Data Separate from each other, as well as the emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:34:13.036012Z",
     "iopub.status.busy": "2022-11-11T09:34:13.035360Z",
     "iopub.status.idle": "2022-11-11T09:34:45.663145Z",
     "shell.execute_reply": "2022-11-11T09:34:45.661855Z",
     "shell.execute_reply.started": "2022-11-11T09:34:13.035977Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating Twitter DF From Json Files\n",
    "twitterDM = \"Twitter/tweets_DM.json\"\n",
    "twitter_df = pd.read_json(twitterDM, lines=True)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:34:45.666383Z",
     "iopub.status.busy": "2022-11-11T09:34:45.665605Z",
     "iopub.status.idle": "2022-11-11T09:35:03.199789Z",
     "shell.execute_reply": "2022-11-11T09:35:03.198661Z",
     "shell.execute_reply.started": "2022-11-11T09:34:45.666316Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting the Tweet Inform From the Source Column of the Original DF\n",
    "twitterDM_df = pd.json_normalize(twitter_df._source, record_prefix=None)\n",
    "twitterDM_df.columns = twitterDM_df.columns.str.replace('^tweet.', '')\n",
    "twitterDM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:03.201496Z",
     "iopub.status.busy": "2022-11-11T09:35:03.201135Z",
     "iopub.status.idle": "2022-11-11T09:35:04.855421Z",
     "shell.execute_reply": "2022-11-11T09:35:04.853997Z",
     "shell.execute_reply.started": "2022-11-11T09:35:03.201463Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating Data_Identification DF\n",
    "twitterID = \"Twitter/data_identification.csv\"\n",
    "twitid_df = pd.read_csv(twitterID)\n",
    "twitid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:04.859500Z",
     "iopub.status.busy": "2022-11-11T09:35:04.858634Z",
     "iopub.status.idle": "2022-11-11T09:35:06.209585Z",
     "shell.execute_reply": "2022-11-11T09:35:06.207912Z",
     "shell.execute_reply.started": "2022-11-11T09:35:04.859455Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating Emotion DF\n",
    "emotion = \"Twitter/emotion.csv\"\n",
    "twitemo_df = pd.read_csv(emotion)\n",
    "twitemo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:06.212287Z",
     "iopub.status.busy": "2022-11-11T09:35:06.211881Z",
     "iopub.status.idle": "2022-11-11T09:35:15.227115Z",
     "shell.execute_reply": "2022-11-11T09:35:15.224069Z",
     "shell.execute_reply.started": "2022-11-11T09:35:06.212247Z"
    }
   },
   "outputs": [],
   "source": [
    "#Combining DataFrames\n",
    "dfs = [twitterDM_df,twitid_df,twitemo_df]\n",
    "twitcom_df = pd.concat([x.set_index('tweet_id') for x in dfs], axis=1).reset_index()\n",
    "twitcom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:15.229849Z",
     "iopub.status.busy": "2022-11-11T09:35:15.229252Z",
     "iopub.status.idle": "2022-11-11T09:35:16.373928Z",
     "shell.execute_reply": "2022-11-11T09:35:16.372557Z",
     "shell.execute_reply.started": "2022-11-11T09:35:15.229789Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extracting the Training and testing Data\n",
    "twitTrain_df = twitcom_df.loc[twitcom_df['identification'] == 'train']\n",
    "twitTest_df = twitcom_df.loc[twitcom_df['identification'] == 'test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes had to be reset for other uses after everything was separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:16.376575Z",
     "iopub.status.busy": "2022-11-11T09:35:16.376043Z",
     "iopub.status.idle": "2022-11-11T09:35:16.401033Z",
     "shell.execute_reply": "2022-11-11T09:35:16.399110Z",
     "shell.execute_reply.started": "2022-11-11T09:35:16.376524Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reseting and Testing Train Dataframe\n",
    "twitTrain_df.reset_index(drop=True, inplace=True)\n",
    "twitTrain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:35:16.404728Z",
     "iopub.status.busy": "2022-11-11T09:35:16.403472Z",
     "iopub.status.idle": "2022-11-11T09:35:16.429745Z",
     "shell.execute_reply": "2022-11-11T09:35:16.428155Z",
     "shell.execute_reply.started": "2022-11-11T09:35:16.404670Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reseting and Testing Test Dataframe\n",
    "twitTest_df.reset_index(drop=True, inplace=True)\n",
    "twitTest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:38:46.813100Z",
     "iopub.status.busy": "2022-11-11T09:38:46.812635Z",
     "iopub.status.idle": "2022-11-11T09:38:46.821626Z",
     "shell.execute_reply": "2022-11-11T09:38:46.820432Z",
     "shell.execute_reply.started": "2022-11-11T09:38:46.813057Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extracting and Testing Training Data\n",
    "tweets_train = twitTrain_df['text']\n",
    "tweets_test = twitTest_df['text']\n",
    "emolabel_train = twitTrain_df['emotion']\n",
    "tweets_train[50], emolabel_train[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "Originally, no preprocessing was done. This was becuase I thought that even without it I would still be able to get a 0.7 or above accuracy. An assumption that proved to be true. With that being said, for some reason or another, I decided to add preprocessing of the data to see how far I could take this. The preprocessing step was mostly to remove usernames, one letter words, unnecessary stopwords, spaces, the lower of text for uniformity and word lemmetimization. I also wanted to do more but I think I did enough. (I got tired). Also tested whether or not removing stop words performed better or worse. You can tell which one I chose based on int's inclusion/exclusion in th immediate following data. Plan to utilize emoji detection as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# PREVIOUS VERSION####################################\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# import re\n",
    "# import emoji \n",
    "\n",
    "# def preprocess_text(Tweet):\n",
    "#         #Removes Username\n",
    "#         Tweet = re.sub('@[^\\s]+','',Tweet)\n",
    "        \n",
    "#         #Demojize Emojis\n",
    "#         Tweet = emoji.demojize(Tweet)\n",
    "#         Tweet = Tweet.replace(\":\",\" \")\n",
    "#         Tweet = ' '.join(Tweet.split())\n",
    "        \n",
    "#         #Removes LH\n",
    "#         Tweet = re.sub('<[^\\s]+','',Tweet)\n",
    "        \n",
    "#         #Removes Hashtage\n",
    "#         Tweet = re.sub('#','',Tweet)\n",
    "        \n",
    "#         # Remove puntuations and numbers\n",
    "#         Tweet = re.sub('[^a-zA-Z]', ' ', Tweet)\n",
    "               \n",
    "#         # Remove single characters\n",
    "#         Tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', Tweet)\n",
    "\n",
    "#         # remove multiple spaces\n",
    "#         Tweet = re.sub(r'\\s+', ' ', Tweet)\n",
    "#         Tweet = Tweet.lower()\n",
    "        \n",
    "#         # Convert Text sentence to Tokens\n",
    "#         Tweet = word_tokenize(Tweet)\n",
    "       \n",
    "#         #Remove unncecessay stopwords #####Disabled#####\n",
    "#         stop_words = stopwords.words('english')\n",
    "#         filtered_text = []\n",
    "#         for t in Tweet:\n",
    "#             #if t not in stop_words:\n",
    "#             filtered_text.append(t)\n",
    "\n",
    "#         # Word lemmatization\n",
    "#         wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#         processed_text1 = []\n",
    "#         for t in filtered_text:\n",
    "#             word1 = wordnet_lemmatizer.lemmatize(t, pos=\"n\")\n",
    "#             word2 = wordnet_lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "#             word3 = wordnet_lemmatizer.lemmatize(word2, pos=(\"a\"))\n",
    "#             processed_text1.append(word3)\n",
    "\n",
    "#         result = \"\"\n",
    "#         for word in processed_text1:\n",
    "#             result = result + word + \" \"\n",
    "#         result = result.rstrip() \n",
    "        \n",
    "#         return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Optimized Pre-Processing\n",
    "After giving up on Pre-Processing aside from Keras, I decide to try it again. The Following method was ahieved through an accidental eureka moment when looking for other Tokenizers and Find Twitter Tokenizer that meshed well with the rest of the code as well as the Keras Module. The follwing model performs better than the previous one by doing everything the previous one did but with inclusion of the emojis themselves. While it may have some very rare symbols issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# UPDATED VERSION WITH BETTER RESULTS###########################\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from cleantext import clean\n",
    "\n",
    "def preprocess_text(Tweet):\n",
    "    #Removes LH\n",
    "    Tweet = re.sub('<[^\\s]+','',Tweet)\n",
    "    \n",
    "    #Removes Hashtage\n",
    "    Tweet = re.sub('#','',Tweet)\n",
    "    \n",
    "    #Removes Numbers\n",
    "    Tweet =re.sub(r'[0-9]+', '', Tweet)\n",
    "        \n",
    "    #Removes Currency\n",
    "    Tweet = clean(Tweet, no_currency_symbols=True, replace_with_currency_symbol=\"\")\n",
    "    \n",
    "    #Tokenizer Using Tweet Tokenizer\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    Tweet_t = tknzr.tokenize(Tweet)\n",
    "    \n",
    "    result = \"\"\n",
    "    for word in Tweet_t:\n",
    "        result = result + word + \" \"\n",
    "    result = result.rstrip() \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pre = []\n",
    "for c in tweets_train:\n",
    "    tweets_train_pre.append(preprocess_text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test_pre = []\n",
    "for c in tweets_test:\n",
    "    tweets_test_pre.append(preprocess_text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_train[50])\n",
    "print(tweets_train_pre[50])\n",
    "print(emolabel_train[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_test[50])\n",
    "print(tweets_test_pre[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Tokenization\n",
    "As mentioned before, certain decision were decided due to the speed of how things could be. Here is where that started, with usage of Tensorflow, Keras, and trying to make code that make things faster but instead made things longer. I decided to go with the Keras Tokenization as I was able to set a limit of words to be used, as well as it being very compatible with padding. This solution performed well as it was able to still provided a relatively decent accuracy predicition overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:38:59.200787Z",
     "iopub.status.busy": "2022-11-11T09:38:59.199970Z",
     "iopub.status.idle": "2022-11-11T09:39:44.304946Z",
     "shell.execute_reply": "2022-11-11T09:39:44.303380Z",
     "shell.execute_reply.started": "2022-11-11T09:38:59.200743Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialization of Tokenization of Tweets using Keras Tokoenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 50000)#Num_words choses due to trial and error.\n",
    "\n",
    "tokenizer.fit_on_texts(tweets_train_pre) \n",
    "\n",
    "print(tokenizer.texts_to_sequences([tweets_train_pre[50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to the model being used works best with a fixed input, data must be padded to be properly utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:39:44.308377Z",
     "iopub.status.busy": "2022-11-11T09:39:44.307414Z",
     "iopub.status.idle": "2022-11-11T09:39:51.897562Z",
     "shell.execute_reply": "2022-11-11T09:39:51.896408Z",
     "shell.execute_reply.started": "2022-11-11T09:39:44.308301Z"
    }
   },
   "outputs": [],
   "source": [
    "#First we need to see how much we need to pad it to get the most of it.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(t.split(' ')) for t in tweets_train]\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.hist(lengths, bins=len(set(lengths)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The histogram shows us the lenght of the sequence of texts and as such we can pad it to have the most included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:39:51.900185Z",
     "iopub.status.busy": "2022-11-11T09:39:51.899051Z",
     "iopub.status.idle": "2022-11-11T09:40:33.756988Z",
     "shell.execute_reply": "2022-11-11T09:40:33.755611Z",
     "shell.execute_reply.started": "2022-11-11T09:39:51.900135Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets_train_pre)\n",
    "tweets_train_pad = pad_sequences(sequences, truncating='post', maxlen=30, padding='post')\n",
    "tweets_train_pad[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels were set and functions were created that could easily convert the emotions to a value to be used in the training of the model and convert those values back to string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:40:33.760128Z",
     "iopub.status.busy": "2022-11-11T09:40:33.759748Z",
     "iopub.status.idle": "2022-11-11T09:40:33.928210Z",
     "shell.execute_reply": "2022-11-11T09:40:33.926627Z",
     "shell.execute_reply.started": "2022-11-11T09:40:33.760090Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = set(emolabel_train)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:40:33.930636Z",
     "iopub.status.busy": "2022-11-11T09:40:33.930129Z",
     "iopub.status.idle": "2022-11-11T09:40:33.942777Z",
     "shell.execute_reply": "2022-11-11T09:40:33.941359Z",
     "shell.execute_reply.started": "2022-11-11T09:40:33.930589Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
    "index_to_classes = dict((v, k) for k, v in classes_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:40:33.944882Z",
     "iopub.status.busy": "2022-11-11T09:40:33.944491Z",
     "iopub.status.idle": "2022-11-11T09:40:33.956898Z",
     "shell.execute_reply": "2022-11-11T09:40:33.955753Z",
     "shell.execute_reply.started": "2022-11-11T09:40:33.944848Z"
    }
   },
   "outputs": [],
   "source": [
    "classes_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:40:33.959559Z",
     "iopub.status.busy": "2022-11-11T09:40:33.959060Z",
     "iopub.status.idle": "2022-11-11T09:40:33.969105Z",
     "shell.execute_reply": "2022-11-11T09:40:33.967880Z",
     "shell.execute_reply.started": "2022-11-11T09:40:33.959512Z"
    }
   },
   "outputs": [],
   "source": [
    "index_to_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:40:33.970975Z",
     "iopub.status.busy": "2022-11-11T09:40:33.970621Z",
     "iopub.status.idle": "2022-11-11T09:40:33.976974Z",
     "shell.execute_reply": "2022-11-11T09:40:33.976093Z",
     "shell.execute_reply.started": "2022-11-11T09:40:33.970944Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function used to convert string values to numeric values to be used in training along with the padded data.\n",
    "names_to_ids = lambda labels: np.array([classes_to_index.get(x) for x in labels])\n",
    "ids_to_names = lambda labels: np.array([index_to_classes.get(x) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T09:41:52.727142Z",
     "iopub.status.busy": "2022-11-11T09:41:52.726267Z",
     "iopub.status.idle": "2022-11-11T09:41:53.234982Z",
     "shell.execute_reply": "2022-11-11T09:41:53.233656Z",
     "shell.execute_reply.started": "2022-11-11T09:41:52.727095Z"
    }
   },
   "outputs": [],
   "source": [
    "train_emolabels = names_to_ids(emolabel_train)\n",
    "print (tweets_train_pre[0])\n",
    "print (emolabel_train[0])\n",
    "print(train_emolabels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL CREATION\n",
    "#### Genuinely just relied on Google for the formatting and trial and error.\n",
    "Model was created based on RNN and utilized the Keras Layers to aid with long-form content by using LSTM to mitigate the initial proplems with RNN which is it's potentially to lean towards improper weight gradients when it comes to large data looping too much and messing with the weight values.\n",
    "With all that being said, even without pre-processing it still performed relatively well. The Original Training of the Model was done at batches of 64, to save time and Epochs of 25. The 64, which is larger than the usual 32, may have been faster but could have impacted accuracy as well. As such, in the second round with the preprocessed data, it was changed to 32 and 10. The model utilize Keras embedding and layers to create an RNN that checks for meaning withing the word and related words of the twitter. The LSTM allows for the RNN to avoid looping too much and messing with the ability to validate data properly. The Dense is amount to amount of emotions and utilize softmax for the predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T10:01:31.315416Z",
     "iopub.status.busy": "2022-11-11T10:01:31.315006Z",
     "iopub.status.idle": "2022-11-11T10:01:32.270458Z",
     "shell.execute_reply": "2022-11-11T10:01:32.269359Z",
     "shell.execute_reply.started": "2022-11-11T10:01:31.315383Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(50000, 16, input_length=30),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model4.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "No validation data sure made things interesting.\n",
    "Original Training = 18/25 Epoch for Batches of 64.\n",
    "Current = 10 Epoch for Batches of 32.\n",
    "There was an issue whereby the predit function did not produce the desired results and I thought the model was wrong. However, it turned out I was using an outdated TensorFlow model function and I have learned the importance of reading a Module information and formatting. The working model produce a single value which is then converted from index_to_ids from the previously created function. The most optimal set of intructions and parameter are what we have now in this document. \n",
    "\n",
    "The various versions were tested on a 1 Epoch basis, and the current model, the No Pre-Processing Model consistently placed the best. In order of Performance, the other ones were Model With Stop Words and Demojize Include and the Last was the Pre-Processing Model that got rid of most things.\n",
    "\n",
    "That being said, while this first model was indeed the best, by working with the other models I was able narrow down the num_words, length, and epochs for more or less the best performance I could do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-11T10:01:42.804462Z",
     "iopub.status.busy": "2022-11-11T10:01:42.804063Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training!\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "history4 = model4.fit(tweets_train_pad, train_emolabels, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2)]\n",
    "                    #validation_data = (X_test, y_test))\n",
    "                   )\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding for Test Data\n",
    "sequences_test = tokenizer.texts_to_sequences(tweets_test_pre)\n",
    "tweets_test_pad = pad_sequences(sequences_test, truncating='post', maxlen=30, padding='post')\n",
    "tweets_test_pad[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predicting\n",
    "pred = np.argmax(model4.predict(tweets_test_pad), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape, tweets_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Prediction index to name and comparing test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_result = ids_to_names(pred)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_test[50])\n",
    "print(pred_result[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'id': twitTest_df.tweet_id, 'emotion': pred_result})\n",
    "my_submission.to_csv('submissionOPP10Epo50Kl30.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Model outperforms other Pre-Processing Models but not the Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
